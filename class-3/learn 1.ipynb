{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e32095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e778a27095ea623bff5c9396c46a50690713e5808e77167f5556b835d50d2062\n"
     ]
    }
   ],
   "source": [
    "# setup.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\") \n",
    "print(os.getenv(\"SERPAPI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66335605",
   "metadata": {},
   "source": [
    "\n",
    "## What is LangChain?\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the LLM application lifecycle through composable components and third-party integrations.\n",
    "\n",
    "Core Components Overview\n",
    "1. Chat Models - The brain of your application\n",
    "2. Prompts - Instructions to guide the model\n",
    "3. Chains - Sequential operations\n",
    "4. Tools - External capabilities\n",
    "5. Agents - Decision-making systems\n",
    "6. Memory - Persistent conversation state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9b1df",
   "metadata": {},
   "source": [
    "1. Chat model setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3993918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework designed to simplify the development of applications that utilize large language models (LLMs). It provides a structured way to build applications that can leverage the capabilities of LLMs for various tasks, such as text generation, question answering, summarization, and more.\n",
      "\n",
      "Key features of LangChain include:\n",
      "\n",
      "1. **Modularity**: LangChain is built with modular components that can be easily combined and customized. This allows developers to create tailored solutions for their specific use cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Simple interaction\n",
    "response = llm.invoke([HumanMessage(content=\"What is LangChain?\")])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b681d3",
   "metadata": {},
   "source": [
    "**2a. Understanding Prompt Templates**\n",
    "\n",
    "Prompts are the interface between human intent and AI understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08aebca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List comprehensions in Python provide a concise way to create lists. They are a syntactic sugar that allows you to generate lists in a single line of code, making them more readable and often more efficient than traditional `for` loops.\n",
      "\n",
      "A basic structure of a list comprehension looks like this:\n",
      "\n",
      "```python\n",
      "[expression for item in iterable if condition]\n",
      "```\n",
      "\n",
      "- **expression**: This is the value to be added to the new list.\n",
      "- **item**: The variable that takes the value of the elements in the iterable.\n",
      "- **iterable**: Any Python iterable (like a list, tuple, or string).\n",
      "- **condition**: (Optional) A filter that only includes items that satisfy a certain condition.\n",
      "\n",
      "### Simple Example\n",
      "\n",
      "Let's say we want to create a list of squares for the numbers from 0 to 9. We can use a list comprehension to do this succinctly:\n",
      "\n",
      "```python\n",
      "squares = [x**2 for x in range(10)]\n",
      "print(squares)\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1. `x**2` is the expression that computes the square of `x`.\n",
      "2. `for x in range(10)` iterates over the numbers from 0 to 9.\n",
      "3. There is no condition in this case, so all values are included.\n",
      "\n",
      "### Output:\n",
      "\n",
      "When you run the above code, you will get:\n",
      "\n",
      "```\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
      "```\n",
      "\n",
      "This example demonstrates how list comprehensions can be used to create a list efficiently and clearly.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful coding assistant specialized in {language}.\"),\n",
    "    (\"human\", \"Explain {concept} in {language} with a simple example.\")\n",
    "])\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Test the chain\n",
    "result = chain.invoke({\n",
    "    \"language\": \"Python\",\n",
    "    \"concept\": \"list comprehensions\"\n",
    "})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5a85d",
   "metadata": {},
   "source": [
    "**2b. Few Shot Prompting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723dc71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dull\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Few-shot examples\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"fast\", \"output\": \"slow\"}\n",
    "]\n",
    "\n",
    "# Create few-shot prompt\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Find the opposite word:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Test it\n",
    "chain = final_prompt | llm\n",
    "result = chain.invoke({\"input\": \"bright\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6afe44",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)**\n",
    "\n",
    "**Understanding LCEL**\n",
    "LCEL takes a declarative approach to building new Runnables from existing Runnables. You describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution.\n",
    "Key LCEL Concepts\n",
    "\n",
    "1. Pipe Operator (|): Chains components together\n",
    "2. RunnableSequence: Sequential execution\n",
    "3. RunnableParallel: Parallel execution\n",
    "4. RunnableLambda: Custom functions\n",
    "\n",
    "\n",
    "#### 3A. LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7268a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a crisp autumn afternoon in the small town of Maplewood when Mrs. Agnes Whittaker burst into the local cafÃ©, her cheeks flushed and her eyes wide with panic. â€œHelp! My cat, Mr. Whiskers, is missing!â€ she exclaimed, clutching her oversized handbag as though it were a lifeline.\n",
      "\n",
      "The cafÃ© patrons looked up from their steaming cups, the aroma of freshly brewed coffee hanging in the air. Mrs. Whittaker was well-known for her flamboyant hats and her fierce love for her fluffy tabby. Mr. Whiskers had become a fixture in the town, often spotted napping on the sun-drenched windowsills or trailing behind Mrs. Whittaker on her morning walks.\n",
      "\n",
      "Detective Lena Hart, who had just finished a slice of pie, set her fork down and stood up. â€œIâ€™ll help you, Mrs. Whittaker,â€ she offered, her curiosity piqued. â€œWhen did you last see him?â€\n",
      "\n",
      "â€œI let him out into the backyard this morning, but he never came back in!â€ Mrs. Whittakerâ€™s voice trembled as she spoke. â€œHe always returns for his lunch.â€\n",
      "\n",
      "â€œLetâ€™s start with the backyard,â€ Detective Hart suggested. The two women hurried back to Mrs. Whittakerâ€™s quaint, flower-laden home, where the sun cast long shadows over the garden.\n",
      "\n",
      "Lena scanned the area, her detectiveâ€™s instincts kicking in. She noticed a cluster of vibrant marigolds at the corner of the garden that seemed to be unusually disturbed. Kneeling down, she brushed aside the petals and discovered a small, half-torn piece of fabric caught on a thorn. â€œDid you happen to lose anything recently, Mrs. Whittaker?â€ she asked, holding up the fabric.\n",
      "\n",
      "Mrs. Whittakerâ€™s eyes narrowed. â€œThat looks likeâ€¦ my favorite scarf! I thought Iâ€™d misplaced it!â€\n",
      "\n",
      "Just then, a rustling sound emerged from the adjacent neighborâ€™s yard. Both women turned in unison as a orange-tinted streak dashed past, disappearing under a fence.\n",
      "\n",
      "â€œThatâ€™s Mr. Whiskers!â€ shouted Mrs. Whittaker, running to the fence. â€œI knew he wouldnâ€™t stray too far!â€\n",
      "\n",
      "Detective Hart followed her closely. Peering through the slats, she saw Mr. Whiskers clamoring up a tree, seemingly engaged in a standoff with a smug-looking squirrel. â€œLooks like heâ€™s having a grand adventure\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define components\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a short {style} story about {topic}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=500)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create chain using LCEL\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\n",
    "    \"style\": \"mystery\",\n",
    "    \"topic\": \"a missing cat\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89704afc",
   "metadata": {},
   "source": [
    "#### 3B. Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d5d1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Sentiment: Positive', 'word_count': 'Word count: 9', 'keywords': 'Keywords: mystery, cat, investigation'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create parallel chains\n",
    "def analyze_sentiment(text):\n",
    "    return f\"Sentiment: Positive\" \n",
    "\n",
    "def count_words(text):\n",
    "    return f\"Word count: {len(text.split())}\"\n",
    "\n",
    "def extract_keywords(text):\n",
    "    return \"Keywords: mystery, cat, investigation\"\n",
    "\n",
    "# Parallel processing\n",
    "analysis_chain = RunnableParallel({\n",
    "    \"sentiment\": analyze_sentiment,\n",
    "    \"word_count\": RunnableLambda(count_words),\n",
    "    \"keywords\": extract_keywords\n",
    "})\n",
    "\n",
    "# Test parallel processing\n",
    "text = \"The mysterious case of the missing cat puzzled everyone.\"\n",
    "results = analysis_chain.invoke(text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab8617",
   "metadata": {},
   "source": [
    "**3C: Complex LCEL Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92de2cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Result:\n",
      "{'summary': 'Summary of: The mysterious case of the missing cat puzzled eve...', 'tags': ['AI', 'LangChain', 'Tutorial'], 'original': 'LangChain is a powerful framework for building LLM applications.', 'processed_text': 'The mysterious case of the missing cat puzzled everyone.'}\n",
      "\n",
      "==================================================\n",
      "DEBUG: Understanding the data flow\n",
      "==================================================\n",
      "After RunnablePassthrough.assign:\n",
      "{'text': 'LangChain is a powerful framework for building LLM applications.', 'processed_text': 'The mysterious case of the missing cat puzzled everyone.'}\n",
      "\n",
      "Now the parallel functions can access both 'text' and 'processed_text' keys!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Define processing functions\n",
    "def preprocess_text(inputs):\n",
    "    # text = inputs[\"text\"].strip().lower()\n",
    "    return text \n",
    "\n",
    "def generate_summary(inputs):\n",
    "    return f\"Summary of: {inputs['processed_text'][:50]}...\"\n",
    "\n",
    "def generate_tags(inputs):\n",
    "    return [\"AI\", \"LangChain\", \"Tutorial\"]\n",
    "\n",
    "# Fixed pipeline\n",
    "pipeline = (\n",
    "    RunnablePassthrough.assign(\n",
    "        processed_text=RunnableLambda(preprocess_text)  # This adds processed_text to the dict\n",
    "    )\n",
    "    | RunnableParallel({\n",
    "        \"summary\": RunnableLambda(generate_summary),\n",
    "        \"tags\": RunnableLambda(generate_tags),\n",
    "        \"original\": lambda x: x[\"text\"],\n",
    "        \"processed_text\": lambda x: x[\"processed_text\"]\n",
    "    })\n",
    ")\n",
    "\n",
    "# Test the pipeline\n",
    "result = pipeline.invoke({\n",
    "    \"text\": \"LangChain is a powerful framework for building LLM applications.\"\n",
    "})\n",
    "\n",
    "print(\"Fixed Result:\")\n",
    "print(result)\n",
    "\n",
    "# Let's also debug the data flow to understand what's happening\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEBUG: Understanding the data flow\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: See what RunnablePassthrough.assign produces\n",
    "step1 = RunnablePassthrough.assign(\n",
    "    processed_text=RunnableLambda(preprocess_text)\n",
    ")\n",
    "\n",
    "intermediate_result = step1.invoke({\n",
    "    \"text\": \"LangChain is a powerful framework for building LLM applications.\"\n",
    "})\n",
    "\n",
    "print(\"After RunnablePassthrough.assign:\")\n",
    "print(intermediate_result)\n",
    "print(\"\\nNow the parallel functions can access both 'text' and 'processed_text' keys!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d2bdc",
   "metadata": {},
   "source": [
    "### Memory and State Management (15 minutes)\n",
    "#### Understanding Memory in LangChain\n",
    "\n",
    "Memory allows your applications to remember previous interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ded2b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Hi Alex! That's great to hear! LangChain is a powerful framework for building applications with language models. What specific aspects of LangChain are you interested in learning about?\n",
      "Response 2: Your name is Alex.\n"
     ]
    }
   ],
   "source": [
    "# exercise_4_memory.py\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# Store for chat histories\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create a simple chatbot with memory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Remember the conversation context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# Add memory to the chain\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Test conversation with memory\n",
    "config = {\"configurable\": {\"session_id\": \"user123\"}}\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"My name is Alex and I'm learning LangChain\"},\n",
    "    config=config\n",
    ")\n",
    "print(\"Response 1:\", response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config=config\n",
    ")\n",
    "print(\"Response 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a181cf6",
   "metadata": {},
   "source": [
    "## Module 5: Tools and External APIs \n",
    "#### Understanding Tools\n",
    "Tools are integral to the functionality of LangChain agents, allowing them to perform a wide range of tasks with precision. These predefined functions can handle both simple and complex operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb25850",
   "metadata": {},
   "source": [
    "**Understanding Decorator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "503c207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "def simple_function():\n",
    "    return \"Hello World\"\n",
    "\n",
    "# This is a plain function\n",
    "result = simple_function()\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ea73e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before function execution\n",
      "Function returned: Hello World\n",
      "After function execution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello World'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Before function execution\")\n",
    "        result = func()\n",
    "        print(f\"Function returned: {result}\") \n",
    "        print(\"After function execution\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def simple_function():\n",
    "    return \"Hello World\"\n",
    "\n",
    "# Now the function is \"decorated\"\n",
    "result = simple_function()\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ffa67",
   "metadata": {},
   "source": [
    "**5A. Creating Custom Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e788cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-29 14:48:33\n",
      "25.00%\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import List\n",
    "import datetime\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current date and time.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "@tool\n",
    "def calculate_percentage(part: float, whole: float) -> str:\n",
    "    \"\"\"Calculate what percentage 'part' is of 'whole'.\"\"\"\n",
    "    if whole == 0:\n",
    "        return \"Cannot divide by zero\"\n",
    "    percentage = (part / whole) * 100\n",
    "    return f\"{percentage:.2f}%\"\n",
    "\n",
    "@tool\n",
    "def word_count(text: str) -> int:\n",
    "    \"\"\"Count the number of words in a text.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Test the tools\n",
    "print(get_current_time.invoke({}))\n",
    "print(calculate_percentage.invoke({\"part\": 25, \"whole\": 100}))\n",
    "print(word_count.invoke({\"text\": \"Hello world, this is a test\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0854c6",
   "metadata": {},
   "source": [
    "**5B. SerpAPI Integration Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01529e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing search tool:\n",
      "Search successful!\n",
      "Results: [{'title': 'Expected growth of data centres drives Nvidia - 28 Aug 2025', 'link': 'https://www.bbc.com/reel/video/p0lzn65h/expected-growth-of-data-centres-drives-nvidia-28-aug-2025', 'source': 'BBC', ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_core.tools import Tool, tool\n",
    "\n",
    "@tool\n",
    "def search_tool(query: str) -> str:\n",
    "    \"\"\"Search Google for current information about a query.\"\"\"\n",
    "    try:\n",
    "        search = SerpAPIWrapper()\n",
    "        return search.run(query)\n",
    "    except Exception as e:\n",
    "        return f\"Search failed: {str(e)}\"\n",
    "\n",
    "# Test the tool\n",
    "print(\"\\n Testing search tool:\")\n",
    "try:\n",
    "    result = search_tool.invoke({\"query\": \"latest AI news 2025\"})\n",
    "    print(\"Search successful!\")\n",
    "    print(\"Results:\", str(result)[:200] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674af38",
   "metadata": {},
   "source": [
    "**5C: Multiple Tool Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87d62909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Tools:\n",
      "- get_current_time: Get the current date and time.\n",
      "- calculator: Safely evaluate mathematical expressions.\n",
      "- text_analyzer: Analyze text and return statistics.\n",
      "- search_tool: Search Google for current information about a query.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Safely evaluate mathematical expressions.\"\"\"\n",
    "    try:\n",
    "        # Only allow basic operations for security\n",
    "        allowed_chars = set('0123456789+-*/.() ')\n",
    "        if all(c in allowed_chars for c in expression):\n",
    "            result = eval(expression)\n",
    "            return str(result)\n",
    "        else:\n",
    "            return \"Invalid expression\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def text_analyzer(text: str) -> dict:\n",
    "    \"\"\"Analyze text and return statistics.\"\"\"\n",
    "    words = text.split()\n",
    "    return {\n",
    "        \"word_count\": len(words),\n",
    "        \"character_count\": len(text),\n",
    "        \"sentence_count\": text.count('.') + text.count('!') + text.count('?'),\n",
    "        \"paragraph_count\": text.count('\\n\\n') + 1\n",
    "    }\n",
    "\n",
    "# Tool collection\n",
    "tools = [get_current_time, calculator, text_analyzer, search_tool]\n",
    "\n",
    "# Display available tools\n",
    "print(\"Available Tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"- {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6e636",
   "metadata": {},
   "source": [
    "### 6. Understanding Agents\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain supports the creation of agents that use LLMs as reasoning engines to determine which actions to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d2cc",
   "metadata": {},
   "source": [
    "**6A. Simple ReAct Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "351e9955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out the current time and also calculate 25% of 200. I will first get the current time.\n",
      "\n",
      "Action: get_current_time  \n",
      "Action Input: None  \u001b[0m\u001b[36;1m\u001b[1;3m2025-08-29 14:51:08\u001b[0m\u001b[32;1m\u001b[1;3mI have obtained the current time. Now, I will calculate 25% of 200.\n",
      "\n",
      "Action: calculator  \n",
      "Action Input: \"200 * 0.25\"  \u001b[0m\u001b[33;1m\u001b[1;3m50.0\u001b[0m\u001b[32;1m\u001b[1;3mI have obtained both the current time and the result of the calculation. \n",
      "\n",
      "Final Answer: The current time is 2025-08-29 14:51:08, and 25% of 200 is 50.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Final Result: The current time is 2025-08-29 14:51:08, and 25% of 200 is 50.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define the ReAct prompt template\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the agent\n",
    "tools = [get_current_time, calculator, search_tool]\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Create agent executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "try:\n",
    "    result = agent_executor.invoke({\n",
    "        \"input\": \"What time is it and what's 25% of 200?\"\n",
    "    })\n",
    "    print(\"\\nFinal Result:\", result[\"output\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abed08d",
   "metadata": {},
   "source": [
    "**6b: Search and Summarize Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7024a736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools available: ['search_tool', 'summarize_text', 'get_current_time']\n",
      "âœ… Agent created successfully!\n",
      "\n",
      "ðŸ”„ Testing agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find current information about LangChain updates for 2025 and then summarize the key points from that information.  \n",
      "Action: search_tool  \n",
      "Action Input: \"LangChain 2025 updates\"  \u001b[0m\u001b[36;1m\u001b[1;3m['LangChain Changelog ; Revision queueing in LangGraph Platform Â· August 27, 2025 ; LangSmith Self-Hosted v0.11 Â· August 18, 2025 ; Trace Mode in LangGraph Studio.', \"For those who've used LangChain extensively, what are its current strengths and weaknesses? Has the library improved significantly over the past ...\", 'LangChain Changelog ; Node-level caching in LangGraph Â· May 29, 2025 ; Deferred nodes in LangGraph Â· May 20, 2025 ; MCP with streamable HTTP transport Â· May 8, 2025.', 'LangGraph Platform GA: Deploy & manage long-running, stateful agents Â· May 16, 2025 ; LangGraph Studio v2: Run and debug production traces locally.', \"800 folks from across the globe gathered in San Francisco for LangChain's first industry conference to hear stories of teams building agents.\", \"We've shipped a number of updates to the LangGraph Python library! These include the following: Performance Enhancements: We've made significant ...\", 'LangChain updates break everything constantly and their migration docs are garbage. Memory usage also goes insane with bigger chains - watched ...', 'LangChain Changelog ; LangGraph v0.4: Working with Interrupts Â· April 29, 2025 ; LangSmith Self-Hosted v0.10 Â· April 22, 2025 ; Catch Production Failures Early with ...', \"Generally, we've reduced LangChain's surface area and are narrowing in on popular and essential abstractions. We've also moved create_react_agent from langgraph ...\"]\u001b[0m\u001b[32;1m\u001b[1;3mI'll summarize the key points from the retrieved information about LangChain updates for 2025.  \n",
      "Action: summarize_text  \n",
      "Action Input: \"LangChain Changelog; Revision queueing in LangGraph Platform Â· August 27, 2025; LangSmith Self-Hosted v0.11 Â· August 18, 2025; Trace Mode in LangGraph Studio. For those who've used LangChain extensively, what are its current strengths and weaknesses? Has the library improved significantly over the past ...; Node-level caching in LangGraph Â· May 29, 2025; Deferred nodes in LangGraph Â· May 20, 2025; MCP with streamable HTTP transport Â· May 8, 2025.; LangGraph Platform GA: Deploy & manage long-running, stateful agents Â· May 16, 2025; LangGraph Studio v2: Run and debug production traces locally. 800 folks from across the globe gathered in San Francisco for LangChain's first industry conference to hear stories of teams building agents. We've shipped a number of updates to the LangGraph Python library! These include the following: Performance Enhancements: We've made significant ...; LangChain updates break everything constantly and their migration docs are garbage. Memory usage also goes insane with bigger chains - watched ...; LangGraph v0.4: Working with Interrupts Â· April 29, 2025; LangSmith Self-Hosted v0.10 Â· April 22, 2025; Catch Production Failures Early with ...; Generally, we've reduced LangChain's surface area and are narrowing in on popular and essential abstractions. We've also moved create_react_agent from langgraph ...\"  \u001b[0m\u001b[33;1m\u001b[1;3m- **Recent Updates:** LangChain has released multiple updates including LangGraph platform improvements, LangSmith Self-Hosted versions, and enhancements to performance and node management.\n",
      "- **Conference Insights:** Over 800 participants attended LangChain's first industry conference in San Francisco, sharing experiences related to building agents.\n",
      "- **Strengths and Weaknesses:** Users have noted significant improvements in LangChain, but also pointed out ongoing issues such as breaking changes in updates and challenges with documentation and memory usage.\n",
      "- **Technical Advancements:** Features such as node-level caching, deferred nodes, and a new Trace Mode in LangGraph Studio have been added to improve functionality.\n",
      "- **Ongoing Development:** The library continues to evolve, focusing on essential abstractions and reducing complexity in the user experience.\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: Key points regarding LangChain updates in 2025 include:\n",
      "\n",
      "1. **Recent Updates:** Several enhancements have been made to LangChain, including improvements to the LangGraph platform and LangSmith Self-Hosted versions.\n",
      "  \n",
      "2. **Conference Insights:** LangChain hosted its first industry conference in San Francisco, attended by over 800 participants who shared experiences in building agents.\n",
      "\n",
      "3. **Strengths and Weaknesses:** Users report significant improvements in LangChain but have noted issues with breaking changes, documentation quality, and memory consumption for larger chains.\n",
      "\n",
      "4. **Technical Advancements:** Key features added include node-level caching, deferred nodes, and a new Trace Mode in LangGraph Studio to improve functionality.\n",
      "\n",
      "5. **Ongoing Development:** The library is evolving to focus on essential abstractions and reduce complexity in the user experience.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "==================================================\n",
      "FINAL SUMMARY:\n",
      "==================================================\n",
      "Key points regarding LangChain updates in 2025 include:\n",
      "\n",
      "1. **Recent Updates:** Several enhancements have been made to LangChain, including improvements to the LangGraph platform and LangSmith Self-Hosted versions.\n",
      "  \n",
      "2. **Conference Insights:** LangChain hosted its first industry conference in San Francisco, attended by over 800 participants who shared experiences in building agents.\n",
      "\n",
      "3. **Strengths and Weaknesses:** Users report significant improvements in LangChain but have noted issues with breaking changes, documentation quality, and memory consumption for larger chains.\n",
      "\n",
      "4. **Technical Advancements:** Key features added include node-level caching, deferred nodes, and a new Trace Mode in LangGraph Studio to improve functionality.\n",
      "\n",
      "5. **Ongoing Development:** The library is evolving to focus on essential abstractions and reduce complexity in the user experience.\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary modules\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "@tool\n",
    "def summarize_text(text: str) -> str:\n",
    "    \"\"\"Summarize a long text into key points.\"\"\"\n",
    "    # Create a summarization chain\n",
    "    summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following text in 3-5 bullet points:\\n\\n{text}\"\n",
    "    )\n",
    "    summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    try:\n",
    "        summary = summary_chain.invoke({\"text\": text[:2000]})  # Limit text length\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"Could not summarize: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current date and time.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# Create enhanced tools list\n",
    "enhanced_tools = [search_tool, summarize_text, get_current_time]\n",
    "\n",
    "print(f\"Tools available: {[tool.name for tool in enhanced_tools]}\")\n",
    "\n",
    "# Create the prompt for the agent\n",
    "prompt_template = \"\"\"You are a helpful assistant with access to tools. Use the tools to help answer questions.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Create enhanced agent\n",
    "try:\n",
    "    enhanced_agent = create_react_agent(llm, enhanced_tools, prompt)\n",
    "    enhanced_executor = AgentExecutor(\n",
    "        agent=enhanced_agent, \n",
    "        tools=enhanced_tools, \n",
    "        verbose=True,\n",
    "        max_iterations=8,\n",
    "        handle_parsing_errors=True  # This helps with parsing errors\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Agent created successfully!\")\n",
    "    \n",
    "    # Test search and summarize\n",
    "    print(\"\\nðŸ”„ Testing agent...\")\n",
    "    result = enhanced_executor.invoke({\n",
    "        \"input\": \"Search for information about LangChain 2025 updates and summarize the key points\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL SUMMARY:\")\n",
    "    print(\"=\"*50)\n",
    "    print(result[\"output\"])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "    print(\"\\nDebugging info:\")\n",
    "    print(f\"LLM type: {type(llm)}\")\n",
    "    print(f\"Tools: {[tool.name for tool in enhanced_tools]}\")\n",
    "    print(f\"Tools types: {[type(tool) for tool in enhanced_tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e36a9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d93b9b52",
   "metadata": {},
   "source": [
    "## 7. Complete Application - OpenAI + SerpAPI Integration (20 minutes)\n",
    "Final Project: Research Assistant Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fca814d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ RESEARCH ASSISTANT DEMO\n",
      "==================================================\n",
      "\n",
      "ðŸ“‹ Query 1: Research the latest developments in quantum computing in 2025 and save the findings\n",
      "------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search_tool` with `{'query': 'latest developments in quantum computing 2025'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['Explore the latest advancements in quantum computing, sensing, and communication with our comprehensive Quantum Technology Monitor 2025.', 'More experiments with logical qubits Â· More specialized hardware/software (as opposed to universal quantum computing) Â· More networking noisy ...', 'They plan rack-mounted modular systems like Forte Enterprise for datacenters and introduce Tempo in 2025 with barium qubits.', 'Build practical, high-impact hybrid applications. Â· Invest in strategic skilling and access to reliable quantum computers for experimentation.', \"The â€œQuantum Index Report 2025â€ charts the technology's momentum, with a comprehensive, data-driven assessment of the state of quantum ...\", 'The breakthroughs from the SQMS Nanofabrication Taskforce bring quantum research closer to the ultimate goal: building scalable, fault-tolerant quantum ...', 'Hardware Milestones: In April 2025, Fujitsu and RIKEN announced a 256-qubit superconducting quantum computer â€“ four times larger than their 2023 ...', 'We are on the path to a new era of quantum computing. We must now prepare for the exponential benefits and risks of quantum technologies.']\u001b[0mâŒ Error: The server had an error while processing your request. Sorry about that!\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ“‹ Query 2: What did you find about quantum computing earlier?\n",
      "------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_research_history_tool` with `{'topic': 'quantum computing'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mNo previous research found.\u001b[0m\u001b[32;1m\u001b[1;3mIt appears that there is no previous research saved on quantum computing. Would you like me to conduct a web search to gather current information on this topic?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "âœ… Result: It appears that there is no previous research saved on quantum computing. Would you like me to conduct a web search to gather current information on this topic?\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ“‹ Query 3: Search for recent AI breakthroughs and summarize the top 3\n",
      "------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_search_tool` with `{'query': 'recent AI breakthroughs 2023'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['The latest annual McKinsey Global Survey on the current state of AI confirms the explosive growth of generative AI (gen AI) tools.', 'From GPT-4 and GANS to XAI and NLP, discover the new technologies in AI and find out how they can be implemented in your business to make a profit.', 'Take Note of These AI Development Trends in 2023 Â· 1. Automated Machine Learning (AutoML) Â· 2. Generative AI Â· 3. Natural Language Processing (NLP).', 'This article highlights the most anticipated breakthroughs in AI technology, focusing on advancements in machine learning, natural language ...', 'With a score of 90.04%, Gemini Ultra was the first model to outperform human experts on MMLU, and achieved a state-of-the-art score of 59.4% on ...', \"Elon Musk's Grok Launch: A new AI model for real-time, current event discussions and engagement. OpenAI's GPTs and Assistants: Customizable AI models for ...\", 'A new version called GPT-4 was released on March 14, 2023, and was used in the Microsoft Bing search engine. Other language models have been released, such as ...', \"This year might go down in history as the year we saw the most AI launches: Meta's LLaMA 2, Google's Bard chatbot and Gemini, Baidu's Ernie Bot, ...\", 'Advancements in robotics took center stage in May 2023 as an AI system enabled robots to autonomously conduct scientific experiments. While ...']\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `save_research` with `{'topic': 'Recent AI Breakthroughs 2023', 'findings': '1. **Gemini Ultra**: This AI model achieved a score of 90.04% on the MMLU benchmark, outperforming human experts and setting a new state-of-the-art score of 59.4%.\\n2. **Generative AI Tools**: The explosive growth of generative AI tools has been confirmed by the latest McKinsey Global Survey, highlighting their increasing adoption in various sectors.\\n3. **Robotics and AI Integration**: In May 2023, advancements in robotics were showcased as an AI system enabled robots to autonomously conduct scientific experiments, marking a significant step in AI applications in research.'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mResearch saved for topic: Recent AI Breakthroughs 2023\u001b[0m\u001b[32;1m\u001b[1;3mHere are the top 3 recent breakthroughs in AI as of 2023:\n",
      "\n",
      "1. **Gemini Ultra**: This AI model achieved a remarkable score of 90.04% on the MMLU benchmark, surpassing human experts and setting a new state-of-the-art score of 59.4%.\n",
      "\n",
      "2. **Generative AI Tools**: The latest McKinsey Global Survey confirms the explosive growth of generative AI tools, indicating their increasing adoption across various sectors.\n",
      "\n",
      "3. **Robotics and AI Integration**: In May 2023, significant advancements were made as an AI system enabled robots to autonomously conduct scientific experiments, representing a major step forward in the application of AI in research.\n",
      "\n",
      "This information has been saved for future reference.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "âœ… Result: Here are the top 3 recent breakthroughs in AI as of 2023:\n",
      "\n",
      "1. **Gemini Ultra**: This AI model achieved a remarkable score of 90.04% on the MMLU benchmark, surpassing human experts and setting a new state-of-the-art score of 59.4%.\n",
      "\n",
      "2. **Generative AI Tools**: The latest McKinsey Global Survey confirms the explosive growth of generative AI tools, indicating their increasing adoption across various sectors.\n",
      "\n",
      "3. **Robotics and AI Integration**: In May 2023, significant advancements were made as an AI system enabled robots to autonomously conduct scientific experiments, representing a major step forward in the application of AI in research.\n",
      "\n",
      "This information has been saved for future reference.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ResearchAssistant:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "        self.search = SerpAPIWrapper()\n",
    "        self.research_history = []\n",
    "        \n",
    "    def web_search(self, query: str) -> str:\n",
    "        \"\"\"Search the web for current information on any topic.\"\"\"\n",
    "        try:\n",
    "            results = self.search.run(query)\n",
    "            return results[:1500]  # Limit result length\n",
    "        except Exception as e:\n",
    "            return f\"Search failed: {str(e)}\"\n",
    "    \n",
    "    def save_research(self, topic: str, findings: str) -> str:\n",
    "        \"\"\"Save research findings for later reference.\"\"\"\n",
    "        research_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"topic\": topic,\n",
    "            \"findings\": findings\n",
    "        }\n",
    "        self.research_history.append(research_entry)\n",
    "        return f\"Research saved for topic: {topic}\"\n",
    "    \n",
    "    def get_research_history(self, topic: str = \"\") -> str:\n",
    "        \"\"\"Retrieve previous research on a topic.\"\"\"\n",
    "        if not self.research_history:\n",
    "            return \"No previous research found.\"\n",
    "        \n",
    "        if topic:\n",
    "            relevant = [r for r in self.research_history if topic.lower() in r['topic'].lower()]\n",
    "            if relevant:\n",
    "                return f\"Previous research on {topic}: {relevant[-1]['findings']}\"\n",
    "            return f\"No previous research found for {topic}\"\n",
    "        \n",
    "        return f\"Total research entries: {len(self.research_history)}\"\n",
    "    \n",
    "    def create_agent(self):\n",
    "        # Create tools using StructuredTool.from_function for better multi-argument support\n",
    "        @tool\n",
    "        def web_search_tool(query: str) -> str:\n",
    "            \"\"\"Search the web for current information on any topic.\"\"\"\n",
    "            return self.web_search(query)\n",
    "        \n",
    "        class SaveResearchInput(BaseModel):\n",
    "            \"\"\"Input schema for saving research findings.\"\"\"\n",
    "            topic: str = Field(description=\"The research topic or subject\")\n",
    "            findings: str = Field(description=\"The research findings or summary to save\")\n",
    "        \n",
    "        def save_research_impl(topic: str, findings: str) -> str:\n",
    "            \"\"\"Implementation function for saving research.\"\"\"\n",
    "            return self.save_research(topic, findings)\n",
    "        \n",
    "        save_research_tool = StructuredTool.from_function(\n",
    "            func=save_research_impl,\n",
    "            name=\"save_research\",\n",
    "            description=\"Save research findings for later reference. Provide both topic and findings.\",\n",
    "            args_schema=SaveResearchInput\n",
    "        )\n",
    "        \n",
    "        @tool\n",
    "        def get_research_history_tool(topic: str = \"\") -> str:\n",
    "            \"\"\"Retrieve previous research on a topic. Leave topic empty to see all research.\"\"\"\n",
    "            return self.get_research_history(topic)\n",
    "        \n",
    "        tools = [web_search_tool, save_research_tool, get_research_history_tool]\n",
    "        \n",
    "        # Use tool-calling agent instead of ReAct agent for better multi-argument tool support\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a research assistant. Help users find information and keep track of research.\n",
    "            \n",
    "When saving research with the save_research tool, always provide both topic and findings as separate arguments.\n",
    "Be thorough in your research and always save important findings for future reference.\"\"\"),\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\")\n",
    "        ])\n",
    "        \n",
    "        agent = create_tool_calling_agent(self.llm, tools, prompt)\n",
    "        \n",
    "        return AgentExecutor(\n",
    "            agent=agent,\n",
    "            tools=tools,\n",
    "            verbose=True,\n",
    "            max_iterations=8,\n",
    "            handle_parsing_errors=True\n",
    "        )\n",
    "\n",
    "# Initialize and test the research assistant\n",
    "assistant = ResearchAssistant()\n",
    "agent = assistant.create_agent()\n",
    "\n",
    "# Test scenarios\n",
    "test_queries = [\n",
    "    \"Research the latest developments in quantum computing in 2025 and save the findings\",\n",
    "    \"What did you find about quantum computing earlier?\",\n",
    "    \"Search for recent AI breakthroughs and summarize the top 3\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ”¬ RESEARCH ASSISTANT DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nðŸ“‹ Query {i}: {query}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        result = agent.invoke({\"input\": query})\n",
    "        print(f\"âœ… Result: {result['output']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
